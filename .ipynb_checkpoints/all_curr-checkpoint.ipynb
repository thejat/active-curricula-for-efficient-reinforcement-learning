{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from maze import Maze\n",
    "import itertools\n",
    "import copy\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def change(Q1, Q2, env):\n",
    "    thres = 0.0 \n",
    "    prev_val = np.sum(Q1)\n",
    "    new_val = np.sum(Q2)\n",
    "    # print (prev_val, new_val)\n",
    "    if(abs(prev_val - new_val) > thres):\n",
    "        change = 1\n",
    "    else:\n",
    "        change = 0\n",
    "    return change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learntask(num, task_num, task, Q, epsilon = 0.3, alpha = 0.6, discount = 0.9, tau = -1):\n",
    "    \n",
    "    grid_size = Q.shape[0]\n",
    "    env = Maze(grid_size, task[:-1], task[-1])\n",
    "    num_actions = env.num_actions\n",
    "    step = 0\n",
    "    episode = 0\n",
    "    exceed = 0\n",
    "    not_change_count = 0\n",
    "    change_no = 5\n",
    "    while ((True and tau == -1) or step < tau):\n",
    "        env.reset()\n",
    "        game_over = False\n",
    "        max_iter = 100\n",
    "        itr = 0\n",
    "        episode += 1\n",
    "        Q2 = copy.deepcopy(Q)\n",
    "        while not (game_over or itr > max_iter):\n",
    "            itr += 1\n",
    "            curr_state = env.state()\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(0, num_actions)\n",
    "            else:\n",
    "                if(np.amax(Q[curr_state[0]][curr_state[1]]) == np.amin(Q[curr_state[0]][curr_state[1]])):\n",
    "                    action = -1#np.random.randint(0, num_actions)\n",
    "                else:\n",
    "                    action = np.argmax(Q[curr_state[0]][curr_state[1]])\n",
    "            next_state, reward, game_over = env.act(action)\n",
    "            step += 1\n",
    "            # Q-learning update\n",
    "            Q[curr_state[0]][curr_state[1]][action] = Q[curr_state[0]][curr_state[1]][action] + alpha*(reward + discount*np.amax(Q[next_state[0]][next_state[1]]) - Q[curr_state[0]][curr_state[1]][action])\n",
    "        if (itr > max_iter):\n",
    "            exceed += 1\n",
    "            not_change_count = 0\n",
    "        elif not change(Q, Q2, env):\n",
    "            not_change_count += 1\n",
    "        if(not_change_count == change_no):\n",
    "            break\n",
    "        else:\n",
    "            not_change_count = 0\n",
    "    # print ('Exceed: %d, Episode: %d' %(exceed, episode))\n",
    "    return [Q, step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subtasks = [[[ 4,0 ] , [ 4,1 ], [ 4,2 ] , [ 4,3 ] , [ 4,4 ]], [[ 4,0 ] , [ 4,1 ] , [ 5,2 ] , [ 6,2] , [ 4,2 ] , [ 4,3 ] , [ 4,4 ]], [[ 4,0 ] , [ 6,6 ] , [ 6,5 ] , [ 6,4 ] , [ 6,3 ] , [ 6,2 ] , [ 4,1 ] , [ 5,2 ] , [ 4,2 ] , [ 4,3 ] , [ 4,4 ]], [[ 0, 0 ],  [ 1,0 ] , [ 2,0 ] , [ 3,0 ] , [ 4,0 ] , [ 6,6 ] , [ 6,5 ] , [ 6,4 ] , [ 6,3 ] , [ 6,2 ] , [ 4,1 ] , [ 5,2 ] , [ 4,2 ] , [ 4,3 ] , [ 4,4 ]]]\n",
    "\n",
    "target_task = [[ 0,0 ] , [ 1,1 ] , [ 1,2 ] , [ 1,3 ] , [ 1,4 ] , [ 1,5 ] , [ 1,0 ] , [ 2,0 ] , [ 3,0 ] , [ 4,0 ] , [ 6,6 ] , [ 6,5 ] , [ 6,4 ] , [ 6,3 ] , [ 6,2 ] , [ 4,1 ] , [ 5,2 ] , [ 4,2 ] , [ 4,3 ] , [ 4,4 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fact(n):\n",
    "    if(n == 1):\n",
    "        return 1\n",
    "    else:\n",
    "        return n*fact(n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "no_tasks = len(subtasks)+1\n",
    "grid_size = 7\n",
    "print (no_tasks)\n",
    "all_steps = np.zeros(fact(no_tasks-1)+1)\n",
    "print all_steps.size\n",
    "Rounds = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round no. 1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-24e00a028deb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#                 # print (step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#             else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearntask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mtot_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mtask_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-fc2eaae38de4>\u001b[0m in \u001b[0;36mlearntask\u001b[0;34m(num, task_num, task, Q, epsilon, alpha, discount, tau)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Q-learning update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mexceed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "for Round in range(Rounds):\n",
    "    print(\"Round no. %d\" % (Round+1))\n",
    "    perm = 0\n",
    "    ind = 0\n",
    "    for T in itertools.permutations(subtasks):\n",
    "        perm += 1\n",
    "        Q = np.zeros((grid_size, grid_size, 4))\n",
    "        tot_step = 0\n",
    "        task_num = 1\n",
    "        for task in T:\n",
    "#             if(task_num == 1):\n",
    "#                 [Q, step] = learntask(perm, task_num, task, Q, grid_size, alpha = 0.2)\n",
    "#                 # print (step)\n",
    "#             else:\n",
    "            [Q, step] = learntask(perm, task_num, task, Q, alpha = 0.2)\n",
    "            tot_step += step\n",
    "            task_num += 1 \n",
    "        # print(tot_step)\n",
    "        [Q, step] = learntask(perm, task_num, target_task, Q, alpha = 0.2)\n",
    "        # print(tot_step)\n",
    "        all_steps[ind] += tot_step\n",
    "        ind += 1\n",
    "        print(tot_step)\n",
    "        # plot_policy(perm, Q)\n",
    "\n",
    "    # BASELINE\n",
    "    print ('baseline')\n",
    "    Q = np.zeros((grid_size, grid_size, 4))\n",
    "    [Q, step] = learntask(0, 0, target_task, Q, alpha = 0.2)\n",
    "    all_steps[ind] += step\n",
    "    print (all_steps[0:ind+1])\n",
    "#     with open(\"verify_new_subtasks.csv\", \"w\") as fp:\n",
    "#         wr = csv.writer(fp)\n",
    "#         for i in all_steps:\n",
    "#             wr.writerow([i/(Round+1)])\n",
    "    # plot_policy(0, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
